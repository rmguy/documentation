Cluster Provisioning
====================

# Overview

- [Overview](#overview)
- [Key Concepts](#key-concepts)
- [Introduction](#Introduction)
- [Provisioning](#Provisioning)

## Introduction

This handbook walks you through provisioning a shard master capable of running suites for many boards, and a shard, capable of coordinating with the master to run suites for a single board. Any machine in the cluster has the same core configuration described by the cluster-core puppet module. This allows us to easily promote a shard to a master and viceversa. If you don't care about scaling, performance, or a distributed testing environment, you can just provision a master and fire suites at it using run_suite, suite_scheduler, test_that or the frontend.

## Key Concepts
![Architecture Diagram](https://cloud.githubusercontent.com/assets/3627706/5236006/ad969c72-77d1-11e4-9281-52b6e913fdb3.png?raw=true "Architecture overview")

To provision any machine for membership to an autotest cluster it must have the following:
* An autotest repo installation
* A mysql installation
* An apache installation
* Update stateful script for repair

Having provisioned an anonymous host with this common denominator of packages, you can easily convert it into a 
db host, drone, afe, full cq instance etc. The prevents us from committing a host for a given use before it is needed in production.

## Provisioning

### Localhost 

The following will provision a cluster on localhost using vagrant.

```
sudo mount --bind <your autotest checkout> /usr/local/autotest
cd /usr/local/autotest/puppylab
clusterctl provision --admin-repo path/to/admin/repo
```

A VagrantFile is autogenerated in the above step, which should give you at least one master and shard.
The default settings should allow you to perform the following operations:

```
vagrant ssh master
sudo su chromeos-test
cd /usr/local/autotest
```

You can ssh into the shard with:
```
vagrant ssh lumpyshard
```

To bootstrap this cluster for testing:
```
/usr/local/autotest/contrib/task_runner.sh
```

This should add a set of hosts to your master and shard. You can confirm this by navigating to the hosts tab on both localhost:8001 and localhost:8004. You need to make sure the following services are running on all vms that will be executing suites:

```
sudo status scheduler; sudo status host-scheduler; sudo /etc/init.d/apache2 status
```

Once you have done this you can fire tests against the master and it will send it to the shard as appropriate, eg:

```
/usr/local/autotest/site_utils/run_suite.py -b link -i link-release/R40-6341.0.0 -s bvt -p bot --web localhost:8001 --no_wait True
```
Will execute a bvt suite against the master *without* hosts. You can watch this suite as it executes through the localhost:8001 afe.

```
/usr/local/autotest/site_utils/run_suite.py -b lumpy -i lumpy-release/R40-6341.0.0 -s bvt -p bot --web localhost:8001 --no_wait True
```

Should do the same against a shard (the suites are still created on the master, and sent to the shard). You can watch this suite as it exexutes through the localhost:8004 afe. To sync code on localhost into all vms in this cluster you need to run:

```
clusterctl provision --sync
```
And restart the right services.


### Ganeti instance
The following will provision a ganeti server as the autotest-cq role. You can replace autotest-cq with any role in the puppet repo.
```
fab -H <hostname> bootstrap
fab -H <hostname> deploy_puppet:server_type=autotest-cq
```
Where the hostname is the FULLLY-qdn.
